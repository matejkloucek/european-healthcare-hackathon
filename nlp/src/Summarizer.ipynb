{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45beb264",
   "metadata": {},
   "source": [
    "## SUMMARIZER\n",
    "* Load our model with learned weights and summarize Czech texts\n",
    "* Model checkpoints are available on https://huggingface.co/ctu-aic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c7fc40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import datasets\n",
    "from datasets import DatasetDict\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "from transformers import MBartForConditionalGeneration\n",
    "from transformers import MBartTokenizerFast\n",
    "\n",
    "from sentence_splitter import SentenceSplitter, split_text_into_sentences\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s | %(name)s | %(levelname)s | %(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0d8572",
   "metadata": {},
   "source": [
    "Install all required dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8c819f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "345c6fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#comment cuda and delete to(cuda) if using cpu\n",
    "class Summarizer:\n",
    "    def __init__(self,model, tokenizer, inference_cfg):\n",
    "        self.model = model\n",
    "        self.model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.inference_cfg = inference_cfg\n",
    "        self.enc_max_len = 1024\n",
    "    \n",
    "    #summarize input text\n",
    "    def __call__(self, texts, inference_cfg=None):\n",
    "        if type(texts) == str:\n",
    "            texts = [texts]\n",
    "        assert type(texts) == list, \"Expected string or list of strings\"\n",
    "        summaries = []\n",
    "        self.inference_cfg = inference_cfg if inference_cfg is not None else self.inference_cfg\n",
    "        for text in texts:\n",
    "            text = self.tokenizer.eos_token.join(SentenceSplitter(language='cs').split(text))\n",
    "            ttext = self.tokenizer(text,max_length = self.enc_max_len, truncation=True, padding=\"max_length\",return_tensors=\"pt\")\n",
    "            print(f\"{len(ttext['input_ids'][0])=}\")\n",
    "            print(f\"{ttext=}\")\n",
    "            summaries.append(self._summarize(ttext,**self.inference_cfg)[0])\n",
    "        return summaries\n",
    "    \n",
    "    #summarize batch of data\n",
    "    def _summarize(self, data, num_beams=1, do_sample=False, \n",
    "                    top_k=50, \n",
    "                    top_p=1.0,\n",
    "                    temperature=1.0,\n",
    "                    repetition_penalty=1.0,\n",
    "                    no_repeat_ngram_size = None,\n",
    "                    max_length=1024,\n",
    "                    min_length=10,\n",
    "                    decode_decoder_ids = False,\n",
    "                    early_stopping = False,**kwargs):\n",
    "        summary = model.generate(input_ids=data[\"input_ids\"],attention_mask=data[\"attention_mask\"],\n",
    "                                    num_beams= num_beams,\n",
    "                                   do_sample= do_sample,\n",
    "                                   top_k=top_k,\n",
    "                                   top_p=top_p,\n",
    "                                   temperature=temperature,\n",
    "                                   repetition_penalty=repetition_penalty,\n",
    "                                   max_length=max_length,\n",
    "                                   min_length=min_length,\n",
    "                                   early_stopping=early_stopping,\n",
    "                                 forced_bos_token_id=tokenizer.lang_code_to_id['cs_CZ'])\n",
    "        return self.tokenizer.batch_decode(summary,skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbaa1e2",
   "metadata": {},
   "source": [
    "### Set inference parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9fa5bd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summ_config():\n",
    "    cfg = OrderedDict([\n",
    "        # summarization model - checkpoint from website\n",
    "        (\"model_name\", \"ctu-aic/mbart-at2h-cs-smesum-2\"),\n",
    "        (\"inference_cfg\", OrderedDict([\n",
    "            (\"num_beams\", 4),\n",
    "            (\"top_k\", 40),\n",
    "            (\"top_p\", 0.92),\n",
    "            (\"do_sample\", True),\n",
    "            (\"temperature\", 0.89),\n",
    "            (\"repetition_penalty\", 1.2),\n",
    "            (\"no_repeat_ngram_size\", None),\n",
    "            (\"early_stopping\", True),\n",
    "        #     (\"max_length\", 96),\n",
    "            (\"min_length\", 10),\n",
    "        ])),\n",
    "        #texts to summarize\n",
    "        (\"text\",\n",
    "            [\n",
    "                'Basketbalisty Nového Jičína dělí jediná výhra od postupu do finále Mattoni NBL.</s>Rovněž ve druhé semifinálové bitvě Národní basketbalové ligy podala děčínská parta heroický výkon, opět to na vítězství nestačilo.</s>Ve velmi kvalitním utkání zvedali aktéři diváky v zaplněné hale ze sedaček, nakonec se z vítězství 86:85 radovali favorizovaní hosté z Nového Jičína.</s>Ti vedou v sérii 2:0 na zápasy a v pátek mohou slavit postup do finále.</s>\"Víme, že teď bude složité soupeře porazit třikrát v řadě, ale oba zápasy byly vyrovnané a v tom třetím nás čeká nová partie.</s>My nic nevzdáváme,\" řekl děčínský trenér Pavel Budínský.</s>Utkání mělo dramatický průběh.</s>Hosté zlomili děčínský odpor až v úplném závěru.</s>V dresu domácích hráli dobře pivoti, avšak roli lídra opět prokázal americký rozehrávač Hatcher.</s>\"Naštěstí on na vše nestačil.</s>Jsem šťastný, že vedeme 2:0.</s>Tentokrát jsme měli asi i štěstí,\" uvedl Zbyněk Choleva, kouč Nového Jičína.</s>Oba týmy se střídaly ve vedení.</s>Závěr však vyšel hostům.</s>\"V klíčových momentech utkání jejich hráči předvedli něco extra.</s>Lídři soupeře nás na konci zlomili,\" dodal trenér Děčína.</s>Děčín – Nový Jičín 85:86 (28:20, 43:42, 67:60)</s>Body: Hatcher 25, Williams 16, P.</s>Houška 13, J.</s>Houška 10 – Muirhead 23, Ubilla 19, Walker 17.</s>Trojky: 25/8:19/9.</s>TH: 19/13:16/9.</s>Doskoky: 34:35.</s>Chyby: 20:18.</s>Rozhodčí: Vyklický, Lukeš, Hošek.</s>Diváci: 1020.</s>Nejlepší hráč: Muirhead (NJ).</s>Stav série: 0:2.'\n",
    "            ]\n",
    "        ),\n",
    "    ])\n",
    "    return cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2dcbe5d",
   "metadata": {},
   "source": [
    "##### Load config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0e7815c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = summ_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53acc990",
   "metadata": {},
   "source": [
    "##### Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0df3bb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MBartForConditionalGeneration.from_pretrained(cfg[\"model_name\"])\n",
    "tokenizer = MBartTokenizerFast.from_pretrained(\"facebook/mbart-large-cc25\", src_lang=\"cs_CZ\", tgt_lang=\"cs_CZ\")\n",
    "summarize = Summarizer(model, tokenizer, cfg[\"inference_cfg\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1210aa9",
   "metadata": {},
   "source": [
    "#### Summarize texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "214597c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cfg[\"text\"] = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "31dcfc47",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(ttext['input_ids'][0])=1024\n",
      "ttext={'input_ids': tensor([[169699,     53,  23916,  ...,      1,      1,      1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0]])}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Následky hospitalizace po OTS.']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize(cfg[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e90fec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
